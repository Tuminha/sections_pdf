"""
In this file we will only extract the bibliographic references from the pdf extraction based on the xml file generated by grobid.
First import the necessary modules
Second define the path to the grobid service
Third check if the GROBID service is already running, and if it already running do not start it again
Fourth send the PDF to GROBID
Fifth save the XML output
Sixth parse the XML output and define the namespace
Seventh find all 'biblStruct' elements in the XML and recursively find all 'p' elements in the 'biblStruct'
Eighth print the paragraph text and the text of any 'ref' elements within the paragraph
Ninth add a newline after the paragraph
"""

# First import the necessary modules
import subprocess
import time
import xml.etree.ElementTree as ET
import requests

# Path to the GROBID service
GROBID_PATH = '/Users/franciscoteixeirabarbosa/projects/test/sections_pdf/grobid'


# First check if the GROBID service is already running 
# and if it already running do not start it again
# Check if the GROBID service is already running
try:
    response = requests.get('http://localhost:8070/api/isalive', timeout=10)
    if response.status_code == 200:
        print("GROBID service is already running.")
    else:
        # Start the GROBID service
        p = subprocess.Popen(['./gradlew', 'run', '--stacktrace'], cwd=GROBID_PATH)
        # Wait for the GROBID service to start
        time.sleep(10)
except requests.exceptions.RequestException as e:
    # If the request fails, it means the service is not running
    print("GROBID service is not running. Starting it now...")
    p = subprocess.Popen(['./gradlew', 'run', '--stacktrace'], cwd=GROBID_PATH)
    # Wait for the GROBID service to start
    time.sleep(10)

# Wait for the GROBID service to start
time.sleep(10)

PDF_PATH = (
    '/Users/franciscoteixeirabarbosa/projects/test/sections_pdf/data/Implant stability change and osseointegration speed of immediately loaded photofunctionalized implants.pdf'
)

# Send the PDF to GROBID
with open(PDF_PATH, 'rb') as f:
    response = requests.post('http://localhost:8070/api/processFulltextDocument', files={'input': f}, timeout=10)

# Save the XML output
with open('output.xml', 'w', encoding='utf-8') as f:
    f.write(response.text)

# Parse the XML output
tree = ET.parse('output.xml')
root = tree.getroot()

# Define the namespace
ns = {'tei': 'http://www.tei-c.org/ns/1.0'}

# Parse the XML file
tree = ET.parse('output.xml')
root = tree.getroot()

# Define the extract_bibliography function
def extract_bibliography(xml_root: ET.Element, namespace: dict) -> None:
    """
    This function extracts the bibliography from the XML root.
    """
    # Find all 'biblStruct' elements in the XML
    for bibl in xml_root.findall('.//tei:biblStruct', namespace):
        analytic = bibl.find('tei:analytic', namespace)
        monogr = bibl.find('tei:monogr', namespace)

        if analytic is not None:
            title = analytic.find('tei:title', namespace)
            authors = analytic.findall('tei:author', namespace)
            author_names = []
            for author in authors:
                persName = author.find('tei:persName', namespace)
                if persName is not None:
                    author_surname = persName.find('tei:surname', namespace)
                    if author_surname is not None:
                        author_names.append(author_surname.text)

        if monogr is not None:
            publication = monogr.find('tei:title', namespace)
            date = monogr.find('tei:imprint/tei:date', namespace)

        if title is not None and publication is not None and date is not None:
            print(f"Title: {title.text}, Authors: {', '.join(author_names)}, Published in: {publication.text}, Date: {date.get('when')}")

# Extract the bibliography
extract_bibliography(root, ns)
